
# ***1***\|***0*****吐槽**


在连续挖了好几个坑之后，又开了一个新的坑：推理优化。它属于一个llm底层的应用，目的是在操作系统层面来优化llm的执行速度进而优化整个模型。
那闲话少说，我们正式开始。


# ***2***\|***0*****llm的过程**


## ***2***\|***1*****prefill阶段与decoding阶段**


prefill
[![](https://img2024.cnblogs.com/blog/2517174/202410/2517174-20241030224752488-799321593.png)](https://img2024.cnblogs.com/blog/2517174/202410/2517174-20241030224752488-799321593.png)
decoding
[![](https://img2024.cnblogs.com/blog/2517174/202410/2517174-20241030224816922-1076394353.png)](https://img2024.cnblogs.com/blog/2517174/202410/2517174-20241030224816922-1076394353.png)
这两者的区别是prefill会先把所有的数据进行拿出来计算，后者只会拿很小一块


# ***3***\|***0*****推理优化的Benchmark**


## ***3***\|***1*****吞吐量**


单位时间内系统能吐出多少个decoding
影响因素：模型优化，输入数据长度


## ***3***\|***2*****First Token Latency（很重要）**


首次prefill阶段所花费的时间
影响因素：输入长度


## ***3***\|***3*****Latency**


生成每个词的间隔
影响因素：输入长度


## ***3***\|***4*****QPS（每秒请求数）**


QPS\=K/这K个请求的时间
影响因素：显卡利用率


# ***4***\|***0*****LLM推理的子过程**


[![](https://img2024.cnblogs.com/blog/2517174/202410/2517174-20241030230349056-1242182033.png)](https://img2024.cnblogs.com/blog/2517174/202410/2517174-20241030230349056-1242182033.png)


# ***5***\|***0*****优化**


1/流水线前后处理与高性能采样
本质是处理过程中的Tokenize和Detokenize部分可以在cpu中进行处理，这样就不必浪费gpu资源进而提升效率。
2/动态批处理
利用流水线思想优化处理过程，通过将多个用户的过程结合在一起来提升处理速度，具体来说在self Attention层分成了Flash attention和Decoding attention两个部分一起处理merge step。
3/Cache现存管理
过去的cache是直接给每一个用户分配一个固定大小的内存，也有优化版本将用户的信息切块来分配，都是存在问题的。ppl中采用的VM Allocator在过去的cache基础上做了修改，会根据用户过去的信息来做一个预测长度。这样可以有效的减少浪费
4/KV Cache量化
Q：什么是量化？
A：将浮点数表示的数据转换为更小的数据类型,如整数或固定点数,从而减少存储空间和计算开销。
[![](https://img2024.cnblogs.com/blog/2517174/202410/2517174-20241031234400004-1595005790.png)](https://img2024.cnblogs.com/blog/2517174/202410/2517174-20241031234400004-1595005790.png)
KV缓存决定了服务器能服务的用户数量，优化缓存就是提升性能
应用在self attation,K,V三个层中
5/矩阵乘法量化
矩阵乘法在模型中花费占比70%以上
[![](https://img2024.cnblogs.com/blog/2517174/202411/2517174-20241101200651236-993497515.png)](https://img2024.cnblogs.com/blog/2517174/202411/2517174-20241101200651236-993497515.png)
量化大体方向
[![](https://img2024.cnblogs.com/blog/2517174/202411/2517174-20241101213713824-1795872657.png)](https://img2024.cnblogs.com/blog/2517174/202411/2517174-20241101213713824-1795872657.png):[FlowerCloud机场](https://hanlianfangzhi.com)


# ***6***\|***0*****int8 VS int4**


int8相比于fp16 加载权重减半，计算时间减半。
int4加载权重会减少的更多，但会多一个解量化的时间，且不减半计算时间。
在服务器中多用int8是因为解量化的时间与计算时间正相关，而服务器中计算占比较大。


\_\_EOF\_\_

![](https://github.com/sandust/gallery/image/499892.html)本文作者：**[Sandust](https://github.com)** 本文链接：[https://github.com/sandust/p/18516756\.html](https://github.com)关于博主：评论和私信会在第一时间回复。或者[直接私信](https://github.com)我。版权声明：本博客所有文章除特别声明外，均采用 [BY\-NC\-SA](https://github.com "BY-NC-SA") 许可协议。转载请注明出处！声援博主：如果您觉得文章对您有帮助，可以点击文章右下角**【[推荐](javascript:void(0);)】**一下。您的鼓励是博主的最大动力！
